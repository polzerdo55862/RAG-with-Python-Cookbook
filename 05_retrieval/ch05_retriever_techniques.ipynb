{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python Packages\n",
    "\n",
    "This notebook uses the following Python packages:\n",
    "\n",
    "- `psycopg2` (PostgreSQL database access)\n",
    "- `requests` (HTTP requests)\n",
    "- `openai` (OpenAI API access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2==2.9.10 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (2.9.10)\n",
      "Requirement already satisfied: requests==2.32.3 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from requests==2.32.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from requests==2.32.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from requests==2.32.3) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from requests==2.32.3) (2025.4.26)\n",
      "Requirement already satisfied: openai==1.82.1 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (1.82.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (2.11.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from openai==1.82.1) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.82.1) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.82.1) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.82.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.82.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.82.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.82.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.82.1) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\lib\\site-packages (from tqdm>4->openai==1.82.1) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2==2.9.10\n",
    "!pip install requests==2.32.3\n",
    "!pip install openai==1.82.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Optimizing Query Results using Metadata Filtering in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metadata_filtering_using_postgresql():\n",
    "    \"\"\"\n",
    "    This function fills a embeddings table with some data\n",
    "    from a text about soccer and american football.\n",
    "\n",
    "    We add the topic as a new metadata field, and finally\n",
    "    query the embeddings table in PostgreSQL but\n",
    "    filter the table first.\n",
    "    \"\"\"\n",
    "    import psycopg2\n",
    "    from openai import OpenAI\n",
    "    import json\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rag_cookbook\",\n",
    "        user=\"rag_cookbook_user\",\n",
    "        password=\"rag_cookbook_user_pw\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # drop the table embeddings_table_with_metadata if exists\n",
    "    cur.execute(\"DROP TABLE IF EXISTS embeddings_table_with_metadata\")\n",
    "\n",
    "    # tag::metadata_filtering_create_table[]\n",
    "    # creates the vector extension if it does not exist\n",
    "    cur.execute(\"\"\"CREATE EXTENSION IF NOT EXISTS vector\"\"\")\n",
    "\n",
    "    # Create table with metadata column\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS embeddings_table_with_metadata (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            text_chunk TEXT,\n",
    "            embedding VECTOR(1536),\n",
    "            metadata JSONB\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    # end::metadata_filtering_create_table[]\n",
    "\n",
    "    # tag::fill_metadata_table_with_sample_data[]\n",
    "    # Define text chunks and metadata\n",
    "    text_chunks = [\n",
    "        {\n",
    "            \"text\": \"Roger Federer has won 20 Grand Slam titles in tennis.\",\n",
    "            \"topic\": \"tennis\",\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"The FIFA World Cup is the most prestigious football tournament.\",\n",
    "            \"topic\": \"football\",\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Serena Williams is one of the greatest tennis players of all time.\",\n",
    "            \"topic\": \"tennis\",\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Lionel Messi has won multiple Ballon d'Or awards in football.\",\n",
    "            \"topic\": \"football\",\n",
    "        },\n",
    "    ]\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Insert text chunks with embeddings and metadata\n",
    "    for chunk in text_chunks:\n",
    "        # Get embedding using OpenAI API\n",
    "        response = client.embeddings.create(\n",
    "            input=chunk[\"text\"],\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        embedding = response.data[0].embedding\n",
    "\n",
    "        metadata = {\"topic\": chunk[\"topic\"]}\n",
    "        cur.execute(\n",
    "            \"INSERT INTO embeddings_table_with_metadata (text_chunk, embedding, metadata) \"\n",
    "            \"VALUES (%s, %s, %s)\",\n",
    "            (chunk[\"text\"], embedding, json.dumps(metadata)),\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    # end::fill_metadata_table_with_sample_data[]\n",
    "\n",
    "    # tag::metadata_filtering_in_action[]\n",
    "    # Query the table with metadata filtering\n",
    "    query = \"Who is the best player?\"\n",
    "    response = client.embeddings.create(\n",
    "        input=chunk[\"text\"],\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "    topic_filter = \"football\"\n",
    "\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT text_chunk, 1 - (embedding <=> %s::vector) AS similarity\n",
    "        FROM embeddings_table_with_metadata\n",
    "        WHERE metadata->>'topic' = %s\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT 5\n",
    "        \"\"\",\n",
    "        (query_embedding, topic_filter),\n",
    "    )\n",
    "    results = cur.fetchall()\n",
    "    # end::metadata_filtering_in_action[]\n",
    "\n",
    "    # Close the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "results = metadata_filtering_using_postgresql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Lionel Messi has won multiple Ballon d'Or awards in football.\",\n",
       "  0.9999992847442627),\n",
       " ('The FIFA World Cup is the most prestigious football tournament.',\n",
       "  0.3422972747925217)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Enhancing Search Results by Extending the Original Query with Generated Pseudo-Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_hypothetical_documents():\n",
    "    \"\"\"\n",
    "    Generate hypothetical documents for the vector search chapter\n",
    "\n",
    "    Returns:\n",
    "        text_chunks (list): List of hypothetical documents\n",
    "    \"\"\"\n",
    "    # tag::generate_hypothetical_documents[]\n",
    "    from pydantic import BaseModel\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    user_query = \"What is the revenue of Company X in 2024?\"\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    class HypotheticalDocuments(BaseModel):\n",
    "        documents: list[str]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. Based on the user query below, generate three hypothetical \n",
    "    text chunks that contain relevant information to answer the query.\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_query},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        response_format=HypotheticalDocuments,\n",
    "    )\n",
    "\n",
    "    hypothetical_documents = completion.choices[0].message.parsed.documents\n",
    "    # end::generate_hypothetical_documents[]\n",
    "\n",
    "    return hypothetical_documents\n",
    "\n",
    "hypothetical_documents = generate_hypothetical_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Company X reported a projected revenue of $2.5 billion for the fiscal year 2024, reflecting a 15% increase compared to the previous year, driven by new product launches and expanded market reach.',\n",
       " 'In its latest quarterly earnings call, Company X management indicated that they anticipate achieving a revenue range between $2.4 billion to $2.6 billion for 2024, largely fueled by growth in their digital services division and strategic partnerships.',\n",
       " \"According to the financial forecast released in January 2024, analysts expect Company X's revenue to grow significantly in 2024, with estimates varying between $2.45 billion and $2.55 billion, taking into account the increasing demand for their innovative solutions in various sectors.\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Improving Search Results with Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_query_prompting():\n",
    "    # tag::multi_query_prompting[]\n",
    "    from openai import OpenAI\n",
    "    from pydantic import BaseModel\n",
    "    import os\n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    question = \"What are the benefits of renewable energy?\"\n",
    "\n",
    "    query_prompt = f\"\"\"You are an AI language model assistant. Your task is \n",
    "    to create three alternative versions of the provided user query to \n",
    "    enhance the retrieval of relevant documents from a vector database. \n",
    "    By offering diverse variations of the query, your goal is to help \n",
    "    mitigate the limitations of distance-based similarity search. Provide \n",
    "    these alternative queries, each on a new line. \n",
    "    \n",
    "    Original query: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    # send the query prompt to OpenAI\n",
    "    class QueryVariations(BaseModel):\n",
    "        queries: list[str]\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query_prompt,\n",
    "            },\n",
    "        ],\n",
    "        response_format=QueryVariations,\n",
    "    )\n",
    "\n",
    "    queries = completion.choices[0].message.parsed.queries\n",
    "    # end::multi_query_prompting[]\n",
    "    return queries\n",
    "\n",
    "queries = multi_query_prompting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does renewable energy contribute to environmental sustainability?',\n",
       " 'What advantages does renewable energy offer over fossil fuels?',\n",
       " 'In what ways can renewable energy improve economic growth and efficiency?']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Addressing Complex Requests by Designing a Query Routing System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_routing_system():\n",
    "    \"\"\"\n",
    "    1. Get the user's query and check with the router\n",
    "    2. The router can decide between functions to use to answer the question\n",
    "        Those functions can be vector search of different index subsets or other tools,\n",
    "        like a T2SQL module to query a SQL database\n",
    "\n",
    "    Returns:\n",
    "        selected_data_sources (list): The selected data sources for each user query\n",
    "    \"\"\"\n",
    "    # tag::query_routing[]\n",
    "    from pydantic import BaseModel\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    user_queries = [\n",
    "        {\n",
    "            \"query\": \"Who is the all-time top scorer in the FIFA World Cup?\",\n",
    "            \"selected_data_source\": None,\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"What are the four Grand Slam tennis tournaments?\",\n",
    "            \"selected_data_source\": None,\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Did Manchester United win their last game?\",\n",
    "            \"selected_data_source\": None,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an exptert at routing a user question to the appropraite data source. \n",
    "    Given a user question choose which of the data sources in list_of_data_sources \n",
    "    is the best to answer the question.\n",
    "    \"\"\"\n",
    "\n",
    "    from typing import Literal\n",
    "    from pydantic import Field\n",
    "\n",
    "    class RouterDecision(BaseModel):\n",
    "        data_source: Literal[\n",
    "            \"general_football_knowledge\",\n",
    "            \"general_tennis_knowledge\",\n",
    "            \"latest_football_results_sql\",\n",
    "        ] = Field(\n",
    "            ..., description=\"The best data source to answer the user's question.\"\n",
    "        )\n",
    "\n",
    "    for user_query in user_queries:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\": user_query[\"query\"]},\n",
    "            ],\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_format=RouterDecision,\n",
    "        )\n",
    "        user_query[\"selected_data_source\"] = completion.choices[\n",
    "            0\n",
    "        ].message.parsed.data_source\n",
    "\n",
    "    # user_queries = [{'query': 'Who is the all-time top scorer in the FIFA World Cup?',\n",
    "    #                  'selected_data_source': 'general_football_knowledge'},\n",
    "    #                 {'query': 'What are the four Grand Slam tennis tournaments?',\n",
    "    #                  'selected_data_source': 'general_tennis_knowledge'},\n",
    "    #                 {'query': 'Did Manchester United win their last game?',\n",
    "    #                  'selected_data_source': 'latest_football_results_sql'}]\n",
    "    # end::query_routing[]\n",
    "\n",
    "    return user_queries\n",
    "\n",
    "user_queries = query_routing_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Who is the all-time top scorer in the FIFA World Cup?',\n",
       "  'selected_data_source': 'general_football_knowledge'},\n",
       " {'query': 'What are the four Grand Slam tennis tournaments?',\n",
       "  'selected_data_source': 'general_tennis_knowledge'},\n",
       " {'query': 'Did Manchester United win their last game?',\n",
       "  'selected_data_source': 'latest_football_results_sql'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Increasing Search Efficiency by Designing an Auto-Merging Retriever (aka Parent Document Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_merging_retriever():\n",
    "    \"\"\"\n",
    "    This function demonstrates how to create a table in PostgreSQL\n",
    "    to store text chunks and their embeddings, and how to insert\n",
    "    those chunks into the table. It also shows how to create a\n",
    "    hypothetical dataset of text chunks that are automatically merged\n",
    "    into parent nodes based on their size.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    # tag::auto_merging_retriever_test_data[]\n",
    "    # Example usage\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/polzerdo55862/RAG-cookbook-datasets/main/\"\n",
    "        \"datasets/text_files/random-text-about-5-different-stories-with-pargraphs.txt\"\n",
    "    )\n",
    "\n",
    "    # Load text content from a given URL\n",
    "    text = requests.get(url).text\n",
    "\n",
    "    leaf_node_size = 250  # size of the leaf nodes in characters\n",
    "    parent_merge = 4  # number of leaf nodes to merge into a parent node\n",
    "    parent_node_size = (\n",
    "        parent_merge * leaf_node_size\n",
    "    )  # size of the parent nodes in characters\n",
    "    text_chunks = []  # list to store the text chunk dictionaries\n",
    "\n",
    "    for leaf_node_id in range(0, len(text) // leaf_node_size, 1):\n",
    "        parent_node_id = leaf_node_id // parent_merge\n",
    "        leaf_chunk_start = leaf_node_id * leaf_node_size\n",
    "        parent_chunk_start = parent_node_id * parent_node_size\n",
    "\n",
    "        text_chunk = {\n",
    "            \"leaf_node_id\": leaf_node_id,\n",
    "            \"leaf_chunk\": text[leaf_chunk_start : leaf_chunk_start + leaf_node_size],\n",
    "            \"parent_node_id\": parent_node_id,\n",
    "            \"parent_chunk\": text[\n",
    "                parent_chunk_start : parent_chunk_start + parent_node_size\n",
    "            ],\n",
    "        }\n",
    "        text_chunks.append(text_chunk)\n",
    "    # end::auto_merging_retriever_test_data[]\n",
    "\n",
    "    import psycopg2\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rag_cookbook\",\n",
    "        user=\"rag_cookbook_user\",\n",
    "        password=\"rag_cookbook_user_pw\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS auto_merging_retriever_text_chunks\")\n",
    "\n",
    "    # tag::auto_merging_retriever_create_table[]\n",
    "    # Create a new table for storing chunks and embeddings\n",
    "    cur.execute(\"\"\"CREATE EXTENSION IF NOT EXISTS vector\"\"\")\n",
    "\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE auto_merging_retriever_text_chunks (\n",
    "            leaf_node_id SERIAL PRIMARY KEY,\n",
    "            leaf_chunk TEXT,\n",
    "            parent_node_id INTEGER,\n",
    "            parent_chunk TEXT,\n",
    "            leaf_chunk_embedding VECTOR(1536)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    # end::auto_merging_retriever_create_table[]\n",
    "\n",
    "    # tag::auto_merging_retriever_insert_data[]\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()  # initialize OpenAI client\n",
    "\n",
    "    # Insert chunks and their embeddings into the table\n",
    "    for text_chunk in text_chunks:\n",
    "        leaf_embedding = (\n",
    "            client.embeddings.create(\n",
    "                input=[text_chunk[\"leaf_chunk\"]], model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO auto_merging_retriever_text_chunks \n",
    "            (leaf_node_id, leaf_chunk, parent_node_id, parent_chunk, leaf_chunk_embedding) \n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                text_chunk[\"leaf_node_id\"],\n",
    "                text_chunk[\"leaf_chunk\"],\n",
    "                text_chunk[\"parent_node_id\"],\n",
    "                text_chunk[\"parent_chunk\"],\n",
    "                leaf_embedding,\n",
    "            ),\n",
    "        )\n",
    "    conn.commit()\n",
    "    # end::auto_merging_retriever_insert_data[]\n",
    "\n",
    "auto_merging_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_merging_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Increasing Search Results by Designing a Sentence Window Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_window_retriever():\n",
    "    \"\"\"\n",
    "    This function is used in chapter 5 as a example how to create a table in postgres\n",
    "    \"\"\"\n",
    "\n",
    "    # tag::sentence_window_retriever_test_data[]\n",
    "    file_path = \"../datasets/text_files/random-text-about-5-different-stories-with-pargraphs.txt\"\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split(\". \")\n",
    "    text_chunks = []  # list to store the text chunk dictionaries\n",
    "\n",
    "    for sentence_id, sentence in enumerate(sentences):\n",
    "        text_chunk = {\n",
    "            \"chunk_id\": sentence_id,\n",
    "            \"chunk\": sentence,\n",
    "        }\n",
    "        text_chunks.append(text_chunk)\n",
    "    # end::sentence_window_retriever_test_data[]\n",
    "\n",
    "    import psycopg2\n",
    "\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rag_cookbook\",\n",
    "        user=\"rag_cookbook_user\",\n",
    "        password=\"rag_cookbook_user_pw\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\",\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS sentence_window_retriever_text_chunks\")\n",
    "\n",
    "    # Create a new table for storing chunks and embeddings\n",
    "    cur.execute(\"\"\"CREATE EXTENSION IF NOT EXISTS vector\"\"\")\n",
    "\n",
    "    # tag::sentence_window_retriever_create_table[]\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE sentence_window_retriever_text_chunks (\n",
    "            chunk_id SERIAL PRIMARY KEY,\n",
    "            chunk TEXT,\n",
    "            chunk_embedding VECTOR(1536)\n",
    "        )\n",
    "    \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    # end::sentence_window_retriever_create_table[]\n",
    "\n",
    "    # tag::sentence_window_retriever_insert_data[]\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()  # initialize OpenAI client\n",
    "    # Insert chunks and their embeddings into the table\n",
    "    for text_chunk in text_chunks:\n",
    "        embedding = (\n",
    "            client.embeddings.create(\n",
    "                input=[text_chunk[\"chunk\"]], model=\"text-embedding-3-small\"\n",
    "            )\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO sentence_window_retriever_text_chunks \n",
    "            (chunk_id, chunk, chunk_embedding) \n",
    "            VALUES (%s, %s, %s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                text_chunk[\"chunk_id\"],\n",
    "                text_chunk[\"chunk\"],\n",
    "                embedding,\n",
    "            ),\n",
    "        )\n",
    "    conn.commit()\n",
    "    # end::sentence_window_retriever_insert_data[]\n",
    "    # tag::sentence_window_retriever_query_data[]\n",
    "    # Query the table\n",
    "    query = \"What is the revenue of Company X in 2024?\"\n",
    "    query_embedding = (\n",
    "        client.embeddings.create(input=[query], model=\"text-embedding-3-small\")\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "    cur.execute(\n",
    "        f\"\"\"\n",
    "        SELECT chunk, 1 - (chunk_embedding <=> '{str(query_embedding)}') AS similarity\n",
    "        FROM sentence_window_retriever_text_chunks\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    )\n",
    "    results = cur.fetchall()\n",
    "    # end::sentence_window_retriever_query_data[]\n",
    "    # Close the connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43msentence_window_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msentence_window_retriever\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpsycopg2\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Connect to PostgreSQL\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag_cookbook\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag_cookbook_user\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrag_cookbook_user_pw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m5432\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m cur = conn.cursor()\n\u001b[32m     36\u001b[39m cur.execute(\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS sentence_window_retriever_text_chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\z004j58u\\repos\\others\\rag-oreily-book\\.venv_retriever\\Lib\\site-packages\\psycopg2\\__init__.py:135\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    132\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    134\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "results = sentence_window_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Improving Search Accuracy with Reranking Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reranking():\n",
    "    \"\"\"\n",
    "    This function is used in chapter 5 as a example how to create\n",
    "    a table in postgres\n",
    "    \"\"\"\n",
    "    # tag::reranking[]\n",
    "    import textwrap\n",
    "    from openai import OpenAI\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    text_chunks = {\n",
    "        1: \"Tesla's Supercharger network and tech lead face rising competition from BYD \"\n",
    "        \"and established automakers.\",\n",
    "        2: \"Tesla's production grows, but price competition threatens its market share.\",\n",
    "        3: \"The automotive industry is shifting to EVs due to climate change and \"\n",
    "        \"regulations.\",\n",
    "        4: \"Semiconductor shortages are disrupting automotive supply chains.\",\n",
    "        5: \"Consumer demand for autonomous driving and advanced tech impacts EV \"\n",
    "        \"competition.\",\n",
    "    }\n",
    "\n",
    "    prompt = textwrap.dedent(\n",
    "        f\"\"\"\"\n",
    "        Query: Will Tesla remain the market leader in electric vehicles?\n",
    "\n",
    "        Documents:\n",
    "            1. {text_chunks[1]}\n",
    "            2. {text_chunks[2]}\n",
    "            3. {text_chunks[3]}\n",
    "            4. {text_chunks[4]}\n",
    "            5. {text_chunks[5]}\n",
    "                             \n",
    "        Instructions:\n",
    "        Please assess the relevance of each document to the query and \n",
    "        provide a relevance score from 1 to 5, where 5 is the most relevant.#\n",
    "                             \n",
    "        Relevance Scores:\n",
    "        \"\"\"\n",
    "    )\n",
    "    # end::reranking[]\n",
    "\n",
    "    client = OpenAI()  # initialize OpenAI client\n",
    "\n",
    "    class RankedDocuments(BaseModel):\n",
    "        text_chunk_ids: list[int]\n",
    "        relevance_scores: list[int]\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format=RankedDocuments,\n",
    "    )\n",
    "\n",
    "    ranked_docs = completion.choices[0].message.parsed\n",
    "    # end::reranking[]\n",
    "    return ranked_docs\n",
    "\n",
    "ranked_docs = reranking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RankedDocuments(text_chunk_ids=[1, 2, 3, 4, 5], relevance_scores=[5, 4, 2, 1, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 Decomposing Complex Queries into Multiple Sub-Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_decomposition():\n",
    "    \"\"\"\n",
    "    Query decomposition using PostgreSQL\n",
    "\n",
    "    Returns:\n",
    "        results (list): List of results\n",
    "    \"\"\"\n",
    "    # tag::query_decomposition_using_postgresql[]\n",
    "    from pydantic import BaseModel\n",
    "    from typing import Optional\n",
    "    from openai import OpenAI\n",
    "\n",
    "    class Question(BaseModel):\n",
    "        question: str\n",
    "        answer: Optional[str] = None\n",
    "\n",
    "    class Questions(BaseModel):\n",
    "        questions: list[Question]\n",
    "\n",
    "    splitter_prompt = \"\"\"\n",
    "    You are a helpful assistant for a RAG chatbot.\n",
    "\n",
    "    Your job is to break down complex questions into simpler ones that are easy to \n",
    "    answer. When the answers to these simpler questions are combined, they should \n",
    "    fully answer the original question. If the question is already simple, leave \n",
    "    it as it is. Handle one question at a time.\n",
    "\n",
    "    Example:\n",
    "        1. Query: Did Microsoft or Google make more money last year?\n",
    "\n",
    "    Decomposed Questions: \n",
    "       1. How much profit did Microsoft make last year?\n",
    "       2. How much profit did Google make last year?\n",
    "    \"\"\"\n",
    "    # end::query_decomposition_using_postgresql[]\n",
    "\n",
    "    # tag::query_decomposition_prompt[]\n",
    "    client = OpenAI()\n",
    "\n",
    "    query = \"What are the benefits of renewable energy compared to fossil fuels?\"\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": splitter_prompt},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_format=Questions,\n",
    "    )\n",
    "\n",
    "    decomposed_questions = completion.choices[0].message.parsed.questions\n",
    "    # end::query_decomposition_prompt[]\n",
    "\n",
    "    return decomposed_questions\n",
    "\n",
    "decomposed_questions = query_decomposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Question(question='What are the benefits of renewable energy?', answer=None),\n",
       " Question(question='What are the benefits of using fossil fuels?', answer=None),\n",
       " Question(question='How does renewable energy impact the environment compared to fossil fuels?', answer=None),\n",
       " Question(question='How does the cost of renewable energy compare to fossil fuels?', answer=None),\n",
       " Question(question='What is the availability and scalability of renewable energy compared to fossil fuels?', answer=None)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposed_questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_retriever",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
